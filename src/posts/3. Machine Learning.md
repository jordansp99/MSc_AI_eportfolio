---
title: "Machine Learning"
slug: "Machine Learning"
image: "./images/image3.jpg"
---
## Module Artefacts

### Unit 1
**Discussion 1 - Initial Post**

Drawing on Metcalf’s (2024) article, which frames Industry 5.0 as a human-centric correction to the techno-economic focus of Industry 4.0, I want to examine a catastrophic information system failure in the public administration sector: the UK’s Windrush scandal. This incident epitomises how implementing Industry 4.0 principles, such as automation and data-driven efficiency, without the ethical and human-centric safeguards of Industry 5.0, can lead to devastating societal harm.

The Windrush scandal involved the wrongful detention, denial of legal rights, and deportation of Commonwealth citizens; these individuals, known as the "Windrush generation", were legal residents and had been for decades (Dearden, 2018).  This was a direct consequence of the Home Office's "hostile environment" policy, which created a data-driven system designed to efficiently identify and penalise undocumented immigrants (Taylor, 2018).  The system operated on flawed logic: a lack of data in the system was treated as proof of illegal status. The burden of proof was shifted onto individuals, who were asked to provide official evidence for every year they had lived in the UK (an impossible demand for people who had arrived as children on their parents’ passports).

 The policy embodied an Industry 4.0 approach, prioritising automated data processing and efficiency above all else (Metcalf, 2024, p. 2), as it aimed to reduce a single metric: the number of undocumented immigrants. It is evident that it lacked the three core pillars of Industry 5.0: human-centricity, resilience, and sustainability (Metcalf, 2024, p. 2). The system was not human-centric, as it dehumanised citizens into data points and shifted an impossible burden of proof onto them. It was not resilient, as its rigid rules demanded a type of official paperwork that was known to be unavailable to the Windrush generation. Finally, it proved socially unsustainable by eroding public trust and causing deep, lasting harm to British communities.  The impact of this information system failure was severe. For the individuals affected, the consequences were life-changing: leading to wrongful detention, deportation, loss of homes, jobs, and access to healthcare, inflicting financial and psychological trauma. The economic cost includes a compensation scheme and the loss of these taxpayers’ economic contributions. Most significantly, the reputational cost to the UK Home Office and the government has been extensive, exposing what an independent review called a "profound institutional failure" (Williams 2020) and creating a legacy of systemic injustice.

**References**

Dearden, N. (2018). How British imperial policies led to the Windrush scandal. Al Jazeera Opinions. Available at: https://www.aljazeera.com/opinions/2018/4/19/how-british-imperial-policies-led-to-the-windrush-scandal (Accessed: 04 August 2025).

Joint Council for the Welfare of Immigrants. (n.d.). Windrush Scandal Explained. Available at https://jcwi.org.uk/reportsbriefings/windrush-scandal-explained/

Metcalf, G.S. (2024) 'An Introduction to Industry 5.0: History, Foundations, and Futures', In: Nousala, S., Metcalf, G., Ing, D. (eds) Industry 4.0 to Industry 5.0. Translational Systems Sciences, vol 41. Springer, Singapore.

Taylor, R. (2018). Impact of ‘Hostile Environment’ Policy: Debate on 14 June 2018 (House of Lords Library Briefing). House of Lords Library.

Williams, W. (2020) Windrush Lessons Learned Review. Available at: https://assets.publishing.service.gov.uk/media/5e74984fd3bf7f4684279faa/6.5577_HO_Windrush_Lessons_Learned_Review_WEB_v2.pdf  (Accessed: 04 August 2025).

**Link to Initial Post**: https://www.my-course.co.uk/mod/forum/discuss.php?d=312016

## Unit 2
The exploratory data analysis (EDA) steps detailed provide a understanding of the auto-mpg dataset, covering data cleaning, statistical properties, and inter-variable relationships.

```python
dataset = pd.read_csv("Unit02 auto-mpg.csv")
print(dataset.head())
print(dataset.describe())
dataset.info()
```
```python
[Out]
mpg             0.457092
cylinders       0.508109
displacement    0.701669
horsepower      1.087326
weight          0.519586
acceleration    0.291587
model year      0.019688
dtype: float64
mpg            -0.515993
cylinders      -1.398199
displacement   -0.778317
horsepower      0.696947
weight         -0.809259
acceleration    0.444234
model year     -1.167446
dtype: float64
```
 **Missing Values:** The 'horsepower' feature was found to contain the non-numeric string `'?'`. Rows with these missing values were dropped, and the 'horsepower' column was successfully converted to the correct `int` data type.
```python
dataset["horsepower"].unique()
```
```python
[Out]
array(['130', '165', '150', '140', '198', '220', '215', '225', '190',
       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',
       '200', '210', '193', '?', '100', '105', '175', '153', '180', '110',
       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',
       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',
       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148',
       '129', '96', '71', '98', '115', '53', '81', '79', '120', '152',
       '102', '108', '68', '58', '149', '89', '63', '48', '66', '139',
       '103', '125', '133', '138', '135', '142', '77', '62', '132', '84',
       '64', '74', '116', '82'], dtype=object)
```
```python
#dropping ?
rows_to_drop_mask = dataset["horsepower"] == "?"
dataset_filtered = dataset[~rows_to_drop_mask]
#convert horsepower to int
dataset_filtered.horsepower = dataset_filtered.horsepower.astype(int)
dataset_filtered.info()
```
**Feature Transformation:** The 'origin' variable was transformed from numerical codes (1, 2, 3) into the categorical labels **America, Europe, and Asia** for improved interpretability.

```python
#replace numeric values with categorical
origin_mapping = {1:"America",
                 2: "Europe",
                 3: "Asia"}
dataset_filtered["origin"] = dataset_filtered["origin"].map(origin_mapping)
```

Estimate skew and kurtosis:
```python
#estimate Skewness
skewness = dataset_filtered.skew(numeric_only=True)
#estimate kurtosis
kurtosis = numerical_cols.kurt(numeric_only=True)

print(skewness)
print (kurtosis)
```
```python
[Out]
mpg             0.457092
cylinders       0.508109
displacement    0.701669
horsepower      1.087326
weight          0.519586
acceleration    0.291587
model year      0.019688
dtype: float64
mpg            -0.515993
cylinders      -1.398199
displacement   -0.778317
horsepower      0.696947
weight         -0.809259
acceleration    0.444234
model year     -1.167446
dtype: float64
```
**Skewness and Kurtosis:** 'horsepower' exhibits a high positive skewness (1.09) and positive kurtosis (0.70), indicating a distribution with many low-value entries and a sharper peak than a normal distribution. Most other features, including 'displacement' (0.70) and 'weight' (0.52), also show a right-skew.
Create a correlation heatmap:

```python
# create correlation heatmap
correlation_matrix = dataset_filtered.corr(numeric_only=True)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix,
            annot=True,
            fmt=".2f",
            cmap='coolwarm',
            linewidths=1.5)

plt.title('Correlation Heatmap of Car Features')
plt.show()
```
![Correlation Heatmap of Car Features](/MSc_AI_eportfolio/images/machine_learning/figure1.png "Correlation Heatmap of Car Features")
*Figure 1*

A scatterplot pairwise visualisation of the relationships between all the numeric variables in the dataset:
```python
#scatterplots
sns.pairplot(dataset_filtered, hue='origin', markers=["o", "s", "D"])

plt.show()
```
![pairplot](/MSc_AI_eportfolio/images/machine_learning/figure2.png "Pairplot")
*Figure 2*


## Reflective Piece: What Have I Learned and How?
