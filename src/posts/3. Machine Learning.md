---
title: "Machine Learning"
slug: "Machine Learning"
image: "./images/image3.jpg"
---
## Module Artefacts

### Unit 1
**Discussion 1 - Initial Post**

Drawing on Metcalf’s (2024) article, which frames Industry 5.0 as a human-centric correction to the techno-economic focus of Industry 4.0, I want to examine a catastrophic information system failure in the public administration sector: the UK’s Windrush scandal. This incident epitomises how implementing Industry 4.0 principles, such as automation and data-driven efficiency, without the ethical and human-centric safeguards of Industry 5.0, can lead to devastating societal harm.

The Windrush scandal involved the wrongful detention, denial of legal rights, and deportation of Commonwealth citizens; these individuals, known as the "Windrush generation", were legal residents and had been for decades (Dearden, 2018).  This was a direct consequence of the Home Office's "hostile environment" policy, which created a data-driven system designed to efficiently identify and penalise undocumented immigrants (Taylor, 2018).  The system operated on flawed logic: a lack of data in the system was treated as proof of illegal status. The burden of proof was shifted onto individuals, who were asked to provide official evidence for every year they had lived in the UK (an impossible demand for people who had arrived as children on their parents’ passports).

 The policy embodied an Industry 4.0 approach, prioritising automated data processing and efficiency above all else (Metcalf, 2024, p. 2), as it aimed to reduce a single metric: the number of undocumented immigrants. It is evident that it lacked the three core pillars of Industry 5.0: human-centricity, resilience, and sustainability (Metcalf, 2024, p. 2). The system was not human-centric, as it dehumanised citizens into data points and shifted an impossible burden of proof onto them. It was not resilient, as its rigid rules demanded a type of official paperwork that was known to be unavailable to the Windrush generation. Finally, it proved socially unsustainable by eroding public trust and causing deep, lasting harm to British communities.  The impact of this information system failure was severe. For the individuals affected, the consequences were life-changing: leading to wrongful detention, deportation, loss of homes, jobs, and access to healthcare, inflicting financial and psychological trauma. The economic cost includes a compensation scheme and the loss of these taxpayers’ economic contributions. Most significantly, the reputational cost to the UK Home Office and the government has been extensive, exposing what an independent review called a "profound institutional failure" (Williams 2020) and creating a legacy of systemic injustice.

**References**

Dearden, N. (2018). How British imperial policies led to the Windrush scandal. Al Jazeera Opinions. Available at: https://www.aljazeera.com/opinions/2018/4/19/how-british-imperial-policies-led-to-the-windrush-scandal (Accessed: 04 August 2025).

Joint Council for the Welfare of Immigrants. (n.d.). Windrush Scandal Explained. Available at https://jcwi.org.uk/reportsbriefings/windrush-scandal-explained/

Metcalf, G.S. (2024) 'An Introduction to Industry 5.0: History, Foundations, and Futures', In: Nousala, S., Metcalf, G., Ing, D. (eds) Industry 4.0 to Industry 5.0. Translational Systems Sciences, vol 41. Springer, Singapore.

Taylor, R. (2018). Impact of ‘Hostile Environment’ Policy: Debate on 14 June 2018 (House of Lords Library Briefing). House of Lords Library.

Williams, W. (2020) Windrush Lessons Learned Review. Available at: https://assets.publishing.service.gov.uk/media/5e74984fd3bf7f4684279faa/6.5577_HO_Windrush_Lessons_Learned_Review_WEB_v2.pdf  (Accessed: 04 August 2025).

**Link to Initial Post**: https://www.my-course.co.uk/mod/forum/discuss.php?d=312016
**Link to Peer responses**: 

## Unit 2
The exploratory data analysis (EDA) steps detailed provide a understanding of the auto-mpg dataset, covering data cleaning, statistical properties, and inter-variable relationships.

```python
dataset = pd.read_csv("Unit02 auto-mpg.csv")
print(dataset.head())
print(dataset.describe())
dataset.info()
```
```
mpg             0.457092
cylinders       0.508109
displacement    0.701669
horsepower      1.087326
weight          0.519586
acceleration    0.291587
model year      0.019688
dtype: float64
mpg            -0.515993
cylinders      -1.398199
displacement   -0.778317
horsepower      0.696947
weight         -0.809259
acceleration    0.444234
model year     -1.167446
dtype: float64
```
 **Missing Values:** The 'horsepower' feature was found to contain the non-numeric string `'?'`. Rows with these missing values were dropped, and the 'horsepower' column was successfully converted to the correct `int` data type.
```python
dataset["horsepower"].unique()
```
```
array(['130', '165', '150', '140', '198', '220', '215', '225', '190',
       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',
       '200', '210', '193', '?', '100', '105', '175', '153', '180', '110',
       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',
       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',
       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148',
       '129', '96', '71', '98', '115', '53', '81', '79', '120', '152',
       '102', '108', '68', '58', '149', '89', '63', '48', '66', '139',
       '103', '125', '133', '138', '135', '142', '77', '62', '132', '84',
       '64', '74', '116', '82'], dtype=object)
```
```python
#dropping ?
rows_to_drop_mask = dataset["horsepower"] == "?"
dataset_filtered = dataset[~rows_to_drop_mask]
#convert horsepower to int
dataset_filtered.horsepower = dataset_filtered.horsepower.astype(int)
dataset_filtered.info()
```
**Feature Transformation:** The 'origin' variable was transformed from numerical codes (1, 2, 3) into the categorical labels **America, Europe, and Asia** for improved interpretability.

```python
#replace numeric values with categorical
origin_mapping = {1:"America",
                 2: "Europe",
                 3: "Asia"}
dataset_filtered["origin"] = dataset_filtered["origin"].map(origin_mapping)
```

Estimate skew and kurtosis:
```python
#estimate Skewness
skewness = dataset_filtered.skew(numeric_only=True)
#estimate kurtosis
kurtosis = numerical_cols.kurt(numeric_only=True)

print(skewness)
print (kurtosis)
```
```
mpg             0.457092
cylinders       0.508109
displacement    0.701669
horsepower      1.087326
weight          0.519586
acceleration    0.291587
model year      0.019688
dtype: float64
mpg            -0.515993
cylinders      -1.398199
displacement   -0.778317
horsepower      0.696947
weight         -0.809259
acceleration    0.444234
model year     -1.167446
dtype: float64
```
**Skewness and Kurtosis:** 'horsepower' exhibits a high positive skewness (1.09) and positive kurtosis (0.70), indicating a distribution with many low-value entries and a sharper peak than a normal distribution. Most other features, including 'displacement' (0.70) and 'weight' (0.52), also show a right-skew.

Create a correlation heatmap:

```python
# create correlation heatmap
correlation_matrix = dataset_filtered.corr(numeric_only=True)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix,
            annot=True,
            fmt=".2f",
            cmap='coolwarm',
            linewidths=1.5)

plt.title('Correlation Heatmap of Car Features')
plt.show()
```
![Correlation Heatmap of Car Features](/MSc_AI_eportfolio/images/machine_learning/figure1.png "Correlation Heatmap of Car Features")
*Figure 1*

A scatterplot pairwise visualisation of the relationships between all the numeric variables in the dataset:
```python
#scatterplots
sns.pairplot(dataset_filtered, hue='origin', markers=["o", "s", "D"])

plt.show()
```
![pairplot](/MSc_AI_eportfolio/images/machine_learning/figure2.png "Pairplot")
*Figure 2*

## Unit 3
This activity includes 4 Exercises in Jupyter Notebooks and to  change variables as needed to observe how the change in data points impacts correlation and regression.

**Ex1:**
```python
# calculate the Pearson's correlation between two variables
from numpy import mean
from numpy import std
from numpy import cov
from numpy.random import randn
from numpy.random import seed
from matplotlib import pyplot as plt
import seaborn as sns

from scipy.stats import pearsonr
# seed random number generator
seed(1)

# prepare data
data1 = 20 * randn(1000) + 100
data2 = data1 + (10 * randn(1000) + 50)

# calculate covariance matrix
covariance = cov(data1, data2)

# calculate Pearson's correlation
corr, _ = pearsonr(data1, data2)

# plot
plt.scatter(data1, data2)
plt.show()

# summarize
print('data1: mean=%.3f stdv=%.3f' % (mean(data1), std(data1)))
print('data2: mean=%.3f stdv=%.3f' % (mean(data2), std(data2)))
print('Covariance: %.3f' % covariance[0][1])
print('Pearsons correlation: %.3f' % corr)
```
![Scatterplot](/MSc_AI_eportfolio/images/machine_learning/figure3.png)
*Figure 3*
```
data1: mean=100.776 stdv=19.620
data2: mean=151.050 stdv=22.358
Covariance: 389.755
Pearsons correlation: 0.888
```
By removing the dependence on data1 and making data2 a completely independent random variable, there is no linear relationship between the two variables, pulling the Pearson's r close to 0.0.

`data2 = 50 * randn(1000) + 150`
![Scatterplot](/MSc_AI_eportfolio/images/machine_learning/figure4.png)
*Figure 4*

**Ex 2**

```python
import matplotlib.pyplot as plt
from scipy import stats

#Create the arrays that represent the values of the x and y axis
x = [5,7,8,7,2,17,2,9,4,11,12,9,6]
y = [99,86,87,88,111,86,103,87,94,78,77,85,86]

#Execute a method that returns some important key values of Linear Regression
slope, intercept, r, p, std_err = stat.linregress(x, y)

# measure the correlation 
corr, _ = stat.pearsonr(x, y)
print('Pearsons correlation: %.3f' % corr)

#Create a function that uses the slope and intercept values to return a new value. 
#This new value represents where on the y-axis the corresponding x value will be placed
def myfunc(x):
  return slope * x + intercept

#Run each value of the x array through the function. This will result in a new array with new values for the y-axis
mymodel = list(map(myfunc, x))

#Draw the original scatter plot & the line of linear regression
plt.scatter(x, y)
plt.plot(x, mymodel)
plt.show()
```

First, it uses stats.linregress to calculate the slope and intercept for the line of best fit. Second, it uses stats.pearsonr to calculate the Pearson correlation coefficient (corr), which numerically quantifies the strength and direction of the linear relationship. A correlation of 
−0.759 indicates a strong negative linear relationship, meaning that as the value of the xvariable increases, the value of the y variable tends to consistently decrease.

**Ex 3**

```python
import pandas
from sklearn import linear_model
df = pandas.read_csv("cars.csv")
X = df[['Weight', 'Volume']]
y = df['CO2']
regr = linear_model.LinearRegression()
regr.fit(X, y)
print(regr.coef_)
predictedCO2 = regr.predict([[3300, 1300]])
print(predictedCO2)
#predict the CO2 emission of a car where the weight is 2300kg, and the volume is 1300cm3:
predictedCO2 = regr.predict([[2300, 1300]])
print(predictedCO2)
```
Exercise 3 uses Multiple Linear Regression to model a car's CO2 emissions based on its weight and engine volume. It first trains a linear model by fitting 'Weight' and 'Volume' (the independent variables) to 'CO2' (the dependent variable). The resulting coefficients are printed to quantify the influence of each feature, and finally, the trained model is used to predict the CO2 emission for two different hypothetical cars with specified weight and volume values.

**Ex 4**

```python
import numpy
from sklearn.metrics import r2_score

x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]
y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]

mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))

speed = mymodel(10)
print(speed)
```
The script uses NumPy to fit a third-degree polynomial model to the given data points of time x and speed y. It then uses this established model to predict and print the expected speed y at the specific time of day x equal to 10.

## Unit 4
Unit 4 provided a practical application of foundational machine learning concepts, specifically focusing on correlation analysis and implementing linear regression using the scikit-learn library. The core exercise involved investigating the relationship between a country's mean population and its mean per capita Gross Domestic Product (GDP) over a two-decade period (2001–2021). This task directly addressed the module learning outcome concerning the applicability and challenges associated with different datasets for machine learning algorithms.

**Data Pre-processing:**
The analysis began with robust data pre-processing using the pandas library on the Global_GDP.csv and Global_Population.csv datasets. An important step was deriving the per capita GDP by element-wise division of the total GDP by the population for each country and year, followed by calculating the mean over the 20-year period (2001-2021).
The code snippet below illustrates the key steps for data loading, filtering, and calculating the independent and dependent variables:

```python
import pandas as pd

gdp_df = pd.read_csv("Global_GDP.csv")
pop_df = pd.read_csv("Global_Population.csv")

# Ensure unique entries and define years of interest
gdp_df.drop_duplicates(subset=['Country Name'], inplace=True, keep='first')
pop_df.drop_duplicates(subset=['Country Name'], inplace=True, keep='first')

years = [str(year) for year in range(2001, 2021)]
cols_to_keep = ['Country Name'] + years

# Filter, set index, and convert data types
gdp_filtered = gdp_df[cols_to_keep].set_index('Country Name')
pop_filtered = pop_df[cols_to_keep].set_index('Country Name')

for col in years:
    gdp_filtered[col] = pd.to_numeric(gdp_filtered[col], errors='coerce')
    pop_filtered[col] = pd.to_numeric(pop_filtered[col], errors='coerce')

# Calculate Per Capita GDP and Means
gdp_per_capita_yearly = gdp_filtered.div(pop_filtered)
mean_gdp_per_capita = gdp_per_capita_yearly.mean(axis=1)
mean_population = pop_filtered.mean(axis=1)

# Create final analysis DataFrame and handle missing values
analysis_df = pd.DataFrame({
    'Mean Population': mean_population,
    'Mean GDP per Capita': mean_gdp_per_capita
})
analysis_df.dropna(inplace=True)
```
Task A: Correlation Analysis
The first task involved investigating the linear relationship between the two key variables through a scatter plot and the calculation of the Pearson Correlation Coefficient.


The scatter plot visually suggested a non-linear or, at best, a very weak linear relationship. The plot showed that countries with the highest mean per capita GDP all had relatively low populations.
![Scatterplot](/MSc_AI_eportfolio/images/machine_learning/figure5.png "Scatterplot")
*Figure 5*

```python
correlation_coefficient = analysis_df['Mean Population'].corr(analysis_df['Mean GDP per Capita'])

print(f"Pearson Correlation Coefficient: {correlation_coefficient:.4f}")

```

```
Pearson Correlation Coefficient: -0.0990
```
This value, being very close to zero, confirms that there is no meaningful linear correlation between a country's mean population and its mean per capita GDP. This finding is significant because it suggests that Linear Regression may not be an appropriate predictive model for this specific relationship

Despite the poor correlation, a Linear Regression model was performed as an exercise to understand how the model fits the data and to derive the coefficients. Mean Population was set as the independent variable (X) and Mean GDP per Capita as the dependent variable (Y).
The scikit-learn library was used to set up and train the linear model:

```python
from sklearn.linear_model import LinearRegression
import numpy as np

# prepare data
X = analysis_df[['Mean Population']] 
y = analysis_df['Mean GDP per Capita']

# create and fit the model
model = LinearRegression()
model.fit(X, y)

# get the results
slope = model.coef_[0]
intercept = model.intercept_

print(f"Linear Regression Results:")
print(f"Slope (Coefficient): {slope:.4f}")
print(f"Y-Intercept: {intercept:.4f}")
print(f"\nThe equation of the regression line is: GDP_per_Capita = {slope:.4f} * Population + {intercept:.4f}")

```
![pairplot](/MSc_AI_eportfolio/images/machine_learning/figure6.png "Linear Regression")
```
Linear Regression Results:
Slope (Coefficient): -0.0000
Y-Intercept: 15253.5630

The equation of the regression line is: GDP_per_Capita = -0.0000 * Population + 15253.5630
```

The near-zero slope 0.000 mathematically confirms the finding from the correlation analysis: changes in a country's population size have a negligible impact on predicting its mean per capita GDP within a linear framework. The regression line, which is nearly horizontal at the value of the intercept, demonstrates the limitations of applying a linear model to non-linearly correlated data.

## Unit 5
 I implemented a method for calculating the Jaccard Distance between pairs of individuals based on their attributes.

 Each person's attributes are listed sequentially, where the first entry represents their Gender ("M" or "F"), and the subsequent entries are various characteristics denoted by codes like 'Y' (Yes), 'P' (Positive/Present), 'N' (No), and 'A' (Absent/Unknown). Since Jaccard Distance is a metric best suited for comparing sets based on shared presence of features, the code uses the convert_to_binary function to standardize the attributes: 'Y' and 'P' are mapped to the binary value 1 (indicating attribute presence), while all other values ('N', 'A', 'M', 'F') are mapped to 0 (indicating absence or being irrelevant for the comparison).

 ```python
 data = {
    "Jack": ["M", "Y", "N", "P", "N", "N", "A"],
    "Mary": ["F", "Y", "N", "P", "A", "P", "N"],
    "Jim":  ["M", "Y", "P", "N", "N", "N", "A"]
}

def convert_to_binary(value):
    if value in ['Y', 'P']:
        return 1
    return 0

def calculate_specific_jaccard_distance(person1_name, person2_name, data_dict):

    # get the full attribute lists
    list1_full = data_dict[person1_name]
    list2_full = data_dict[person2_name]
    
   # ignore gender as it's symetric
    list1 = list1_full[1:]
    list2 = list2_full[1:]
    
    f11, f10, f01 = 0, 0, 0
    
    for val1, val2 in zip(list1, list2):
        bin1 = convert_to_binary(val1)
        bin2 = convert_to_binary(val2)
        
        if bin1 == 1 and bin2 == 1:
            f11 += 1
        elif bin1 == 1 and bin2 == 0:
            f10 += 1
        elif bin1 == 0 and bin2 == 1:
            f01 += 1
        
        
    numerator = f01 + f10
    denominator = f01 + f10 + f11
    
    if denominator == 0:
        return 0.0
        
    return numerator / denominator

pairs_to_compare = [
    ("Jack", "Mary"),
    ("Jack", "Jim"),
    ("Jim", "Mary")
]

# print the results
print("Jaccard Distance Calculations:\n")

for p1, p2 in pairs_to_compare:
    coefficient = calculate_specific_jaccard_distance(p1, p2, data)
    print(f"({p1}, {p2}): {round(coefficient, 2)}")
```

```
Jaccard Distance Calculations:

(Jack, Mary): 0.33
(Jack, Jim): 0.67
(Jim, Mary): 0.75
```

## Unit 6
**Task A**

K-Means clustering analysis performed on the Iris dataset. The process involved determining the optimal number of clusters (K) using the Elbow Method and comparing the resulting clusters (at K=3) to the actual species labels.

The first step employed the Elbow Method to find the ideal number of clusters. This method plots the Within-Cluster Sum of Squares (WCSS) against the number of clusters (K). The plot, as shown in Figure 7, reveals a significant "elbow" at K=3, where the rate of WCSS reduction sharply decreases. This confirms that 3 is the optimal number of clusters, aligning with the three known species in the Iris dataset.

![Elbow Method](/MSc_AI_eportfolio/images/machine_learning/figure7.png "Elbow Method")
*Figure 7*

K-Means clustering was executed with (K=3)

Left Plot (K-Means Predicted Clusters): Shows the three clusters identified by the algorithm (Cluster 1, Cluster 2, Cluster 3) and their centroids.
Right Plot (Labels): Shows the data points colored by their true species: Iris-setosa, Iris-versicolour, and Iris-virginica.

![K-Means predicted vs Actual](/MSc_AI_eportfolio/images/machine_learning/figure8.png "K-Means predicted vs Actual")
*Figure 8*

The clusters show a strong correlation with the actual labels. Cluster 2 (red in K-Means plot) aligns nearly perfectly with Iris-setosa (red in Labels plot). Cluster 1 (blue) and Cluster 3 (green) generally correspond to Iris-versicolour (blue) and Iris-virginica (green), respectively. The minor overlap between these two predicted clusters reflects the natural proximity and overlap observed between the Versicolour and Virginica species in the true labels plot. The clustering successfully uncovered the inherent structure of the dataset.

**Task B**

Task B, which utilised the Wine dataset. The analysis followed three main steps: determining the optimal K, performing clustering at K=3, and evaluating the results using performance metrics and visualisations.

The initial step was to find the optimal number of clusters (K) by plotting the Within-Cluster Sum of Squares (WCSS). The WCSS plot, figure 9 displays a sharp drop in WCSS up to K=3, beyond K=3 the curve flattens significantly, forming a clear "elbow." This strongly indicates that K=3 is the appropriate number of clusters, which aligns with the three distinct types of wine (labels 1, 2, and 3) present in the dataset.

![K-Means predicted vs Actual](/MSc_AI_eportfolio/images/machine_learning/figure9.png "Elbow Method")
*Figure 9*

The model achieved exceptional performance metrics:
K-Means was executed with K=3 on the standardised features. Since K-Means assigns arbitrary cluster IDs, a label mapping technique (using the mode of the true labels within each cluster) was applied to align the predicted clusters with the true wine labels (1, 2, 3) for accurate evaluation.

Accuracy Score: 0.9663
High precision, recall, and F1-scores (all ≥ 0.95) across all three wine classes (1, 2, and 3), indicating the model successfully separated the wine types.

| Class | Precision | Recall | F1-Score | Support |
| :---: | :---: | :---: | :---: | :---: |
| **1** | 0.95 | 1.00 | 0.98 | 59 |
| **2** | 1.00 | 0.92 | 0.96 | 71 |
| **3** | 0.94 | 1.00 | 0.97 | 48 |
| | | | | |
| **Accuracy** | | | **0.97** | 178 |
| **Macro Avg** | 0.96 | 0.97 | 0.97 | 178 |
| **Weighted Avg** | 0.97 | 0.97 | 0.97 | 178 |

Confusion Matrix: The heatmap (figure 10) shows the high number of correct predictions along the diagonal, confirming the low misclassification rate. Only minor confusion is seen between the wine types.

![Wine Confusion Matrix](/MSc_AI_eportfolio/images/machine_learning/figure10.png "Wine Confusion Matrix")
*Figure 10*

PCA Scatter Plots: Principal Component Analysis (PCA) was used to reduce the 13 features to two components for a visual comparison. The side-by-side plots (figure 11) demonstrate:
Left Plot (True Labels): Shows the three distinct, well-separated wine groups.
Right Plot (K-Means Clustered Labels): Shows that the clusters identified by K-Means almost perfectly replicate the structure and separation of the true labels.

![PCA Scatter Plots](/MSc_AI_eportfolio/images/machine_learning/figure11.png "PCA Scatter Plots")
*Figure 11*

Python Code to produce plots:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.decomposition import PCA
from scipy.stats import mode
wine = pd.read_csv("Unit06 wine.csv")
X = wine.drop('Wine',axis=1)
y_true= wine['Wine']

#elbow method for idea number of k

wcss = []
for i in range (1,11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot the elbow method graph
plt.figure(figsize=(10, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method')
plt.xlabel('Number of clusters (K)')
plt.ylabel('WCSS')
plt.show()


# scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# perform K-Means clustering with k=3
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)
kmeans.fit(X_scaled)
y_pred_raw = kmeans.labels_

# map cluster labels to the true labels
labels = np.zeros_like(y_pred_raw)
for i in range(3):
    mask = (y_pred_raw == i)
    labels[mask] = mode(y_true[mask], keepdims=True)[0]

# 'labels' now holds the re-mapped cluster predictions
y_pred = labels

# evaluate the clustering performance
accuracy = accuracy_score(y_true, y_pred)
class_report = classification_report(y_true, y_pred)

print(f"Accuracy Score: {accuracy:.4f}\n")
print("Classification Report:")
print(class_report)

# plot the confusion matrix
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y_true),
            yticklabels=np.unique(y_true))
plt.title('Confusion Matrix for K-Means Clustering', fontsize=16)
plt.xlabel('Predicted Label', fontsize=12)
plt.ylabel('True Label', fontsize=12)
plt.show()

# perform PCA to reduce to 2 components for visualisation
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Create a DataFrame with the PCA components and labels
pca_df = pd.DataFrame(data=X_pca, columns=['Principal Component 1', 'Principal Component 2'])
pca_df['True Label'] = y_true
pca_df['Clustered Label'] = y_pred

# plot the clustered and the true labels side by side using PCA components
plt.figure(figsize=(14, 6))

# plotting the true labels
plt.subplot(1, 2, 1)
sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue='True Label', data=pca_df, palette='viridis', s=100)
plt.title('True Labels (PCA)', fontsize=14)
plt.xlabel('Principal Component 1', fontsize=12)
plt.ylabel('Principal Component 2', fontsize=12)
plt.grid(True)

# plotting the clustered labels
plt.subplot(1, 2, 2)
sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue='Clustered Label', data=pca_df, palette='viridis', s=100)
plt.title('K-Means Clustered Labels (PCA)', fontsize=14)
plt.xlabel('Principal Component 1', fontsize=12)
plt.ylabel('Principal Component 2', fontsize=12)
plt.grid(True)

plt.suptitle('Comparison of True and Clustered Labels using PCA', fontsize=18)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```
## Reflective Piece: What Have I Learned and How?
