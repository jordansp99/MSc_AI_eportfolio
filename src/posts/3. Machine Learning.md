---
title: "Machine Learning"
slug: "Machine Learning"
image: "./images/image3.jpg"
---
## Module Artefacts

### Unit 1
**Discussion 1 - Initial Post**

Drawing on Metcalf’s (2024) article, which frames Industry 5.0 as a human-centric correction to the techno-economic focus of Industry 4.0, I want to examine a catastrophic information system failure in the public administration sector: the UK’s Windrush scandal. This incident epitomises how implementing Industry 4.0 principles, such as automation and data-driven efficiency, without the ethical and human-centric safeguards of Industry 5.0, can lead to devastating societal harm.

The Windrush scandal involved the wrongful detention, denial of legal rights, and deportation of Commonwealth citizens; these individuals, known as the "Windrush generation", were legal residents and had been for decades (Dearden, 2018).  This was a direct consequence of the Home Office's "hostile environment" policy, which created a data-driven system designed to efficiently identify and penalise undocumented immigrants (Taylor, 2018).  The system operated on flawed logic: a lack of data in the system was treated as proof of illegal status. The burden of proof was shifted onto individuals, who were asked to provide official evidence for every year they had lived in the UK (an impossible demand for people who had arrived as children on their parents’ passports).

 The policy embodied an Industry 4.0 approach, prioritising automated data processing and efficiency above all else (Metcalf, 2024, p. 2), as it aimed to reduce a single metric: the number of undocumented immigrants. It is evident that it lacked the three core pillars of Industry 5.0: human-centricity, resilience, and sustainability (Metcalf, 2024, p. 2). The system was not human-centric, as it dehumanised citizens into data points and shifted an impossible burden of proof onto them. It was not resilient, as its rigid rules demanded a type of official paperwork that was known to be unavailable to the Windrush generation. Finally, it proved socially unsustainable by eroding public trust and causing deep, lasting harm to British communities.  The impact of this information system failure was severe. For the individuals affected, the consequences were life-changing: leading to wrongful detention, deportation, loss of homes, jobs, and access to healthcare, inflicting financial and psychological trauma. The economic cost includes a compensation scheme and the loss of these taxpayers’ economic contributions. Most significantly, the reputational cost to the UK Home Office and the government has been extensive, exposing what an independent review called a "profound institutional failure" (Williams 2020) and creating a legacy of systemic injustice.

**References**

Dearden, N. (2018). How British imperial policies led to the Windrush scandal. Al Jazeera Opinions. Available at: https://www.aljazeera.com/opinions/2018/4/19/how-british-imperial-policies-led-to-the-windrush-scandal (Accessed: 04 August 2025).

Joint Council for the Welfare of Immigrants. (n.d.). Windrush Scandal Explained. Available at https://jcwi.org.uk/reportsbriefings/windrush-scandal-explained/

Metcalf, G.S. (2024) 'An Introduction to Industry 5.0: History, Foundations, and Futures', In: Nousala, S., Metcalf, G., Ing, D. (eds) Industry 4.0 to Industry 5.0. Translational Systems Sciences, vol 41. Springer, Singapore.

Taylor, R. (2018). Impact of ‘Hostile Environment’ Policy: Debate on 14 June 2018 (House of Lords Library Briefing). House of Lords Library.

Williams, W. (2020) Windrush Lessons Learned Review. Available at: https://assets.publishing.service.gov.uk/media/5e74984fd3bf7f4684279faa/6.5577_HO_Windrush_Lessons_Learned_Review_WEB_v2.pdf  (Accessed: 04 August 2025).

**Link to Initial Post**: https://www.my-course.co.uk/mod/forum/discuss.php?d=312016

## Unit 2
The exploratory data analysis (EDA) steps detailed provide a understanding of the auto-mpg dataset, covering data cleaning, statistical properties, and inter-variable relationships.

```python
dataset = pd.read_csv("Unit02 auto-mpg.csv")
print(dataset.head())
print(dataset.describe())
dataset.info()
```
```
mpg             0.457092
cylinders       0.508109
displacement    0.701669
horsepower      1.087326
weight          0.519586
acceleration    0.291587
model year      0.019688
dtype: float64
mpg            -0.515993
cylinders      -1.398199
displacement   -0.778317
horsepower      0.696947
weight         -0.809259
acceleration    0.444234
model year     -1.167446
dtype: float64
```
 **Missing Values:** The 'horsepower' feature was found to contain the non-numeric string `'?'`. Rows with these missing values were dropped, and the 'horsepower' column was successfully converted to the correct `int` data type.
```python
dataset["horsepower"].unique()
```
```
array(['130', '165', '150', '140', '198', '220', '215', '225', '190',
       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',
       '200', '210', '193', '?', '100', '105', '175', '153', '180', '110',
       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',
       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',
       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148',
       '129', '96', '71', '98', '115', '53', '81', '79', '120', '152',
       '102', '108', '68', '58', '149', '89', '63', '48', '66', '139',
       '103', '125', '133', '138', '135', '142', '77', '62', '132', '84',
       '64', '74', '116', '82'], dtype=object)
```
```python
#dropping ?
rows_to_drop_mask = dataset["horsepower"] == "?"
dataset_filtered = dataset[~rows_to_drop_mask]
#convert horsepower to int
dataset_filtered.horsepower = dataset_filtered.horsepower.astype(int)
dataset_filtered.info()
```
**Feature Transformation:** The 'origin' variable was transformed from numerical codes (1, 2, 3) into the categorical labels **America, Europe, and Asia** for improved interpretability.

```python
#replace numeric values with categorical
origin_mapping = {1:"America",
                 2: "Europe",
                 3: "Asia"}
dataset_filtered["origin"] = dataset_filtered["origin"].map(origin_mapping)
```

Estimate skew and kurtosis:
```python
#estimate Skewness
skewness = dataset_filtered.skew(numeric_only=True)
#estimate kurtosis
kurtosis = numerical_cols.kurt(numeric_only=True)

print(skewness)
print (kurtosis)
```
```
mpg             0.457092
cylinders       0.508109
displacement    0.701669
horsepower      1.087326
weight          0.519586
acceleration    0.291587
model year      0.019688
dtype: float64
mpg            -0.515993
cylinders      -1.398199
displacement   -0.778317
horsepower      0.696947
weight         -0.809259
acceleration    0.444234
model year     -1.167446
dtype: float64
```
**Skewness and Kurtosis:** 'horsepower' exhibits a high positive skewness (1.09) and positive kurtosis (0.70), indicating a distribution with many low-value entries and a sharper peak than a normal distribution. Most other features, including 'displacement' (0.70) and 'weight' (0.52), also show a right-skew.

Create a correlation heatmap:

```python
# create correlation heatmap
correlation_matrix = dataset_filtered.corr(numeric_only=True)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix,
            annot=True,
            fmt=".2f",
            cmap='coolwarm',
            linewidths=1.5)

plt.title('Correlation Heatmap of Car Features')
plt.show()
```
![Correlation Heatmap of Car Features](/MSc_AI_eportfolio/images/machine_learning/figure1.png "Correlation Heatmap of Car Features")
*Figure 1*

A scatterplot pairwise visualisation of the relationships between all the numeric variables in the dataset:
```python
#scatterplots
sns.pairplot(dataset_filtered, hue='origin', markers=["o", "s", "D"])

plt.show()
```
![pairplot](/MSc_AI_eportfolio/images/machine_learning/figure2.png "Pairplot")
*Figure 2*

## Unit 3
This activity includes 4 Exercises in Jupyter Notebooks and to  change variables as needed to observe how the change in data points impacts correlation and regression.

*Ex1:*
```python
# calculate the Pearson's correlation between two variables
from numpy import mean
from numpy import std
from numpy import cov
from numpy.random import randn
from numpy.random import seed
from matplotlib import pyplot as plt
import seaborn as sns

from scipy.stats import pearsonr
# seed random number generator
seed(1)

# prepare data
data1 = 20 * randn(1000) + 100
data2 = data1 + (10 * randn(1000) + 50)

# calculate covariance matrix
covariance = cov(data1, data2)

# calculate Pearson's correlation
corr, _ = pearsonr(data1, data2)

# plot
plt.scatter(data1, data2)
plt.show()

# summarize
print('data1: mean=%.3f stdv=%.3f' % (mean(data1), std(data1)))
print('data2: mean=%.3f stdv=%.3f' % (mean(data2), std(data2)))
print('Covariance: %.3f' % covariance[0][1])
print('Pearsons correlation: %.3f' % corr)
```
![Scatterplot](/MSc_AI_eportfolio/images/machine_learning/figure3.png)
*Figure 3*
```
data1: mean=100.776 stdv=19.620
data2: mean=151.050 stdv=22.358
Covariance: 389.755
Pearsons correlation: 0.888
```
By removing the dependence on data1 and making data2 a completely independent random variable, there is no linear relationship between the two variables, pulling the Pearson's r close to 0.0.

`data2 = 50 * randn(1000) + 150`
![Scatterplot](/MSc_AI_eportfolio/images/machine_learning/figure4.png)

*Ex 2*

```python
import matplotlib.pyplot as plt
from scipy import stats

#Create the arrays that represent the values of the x and y axis
x = [5,7,8,7,2,17,2,9,4,11,12,9,6]
y = [99,86,87,88,111,86,103,87,94,78,77,85,86]

#Execute a method that returns some important key values of Linear Regression
slope, intercept, r, p, std_err = stat.linregress(x, y)

# measure the correlation 
corr, _ = stat.pearsonr(x, y)
print('Pearsons correlation: %.3f' % corr)

#Create a function that uses the slope and intercept values to return a new value. 
#This new value represents where on the y-axis the corresponding x value will be placed
def myfunc(x):
  return slope * x + intercept

#Run each value of the x array through the function. This will result in a new array with new values for the y-axis
mymodel = list(map(myfunc, x))

#Draw the original scatter plot & the line of linear regression
plt.scatter(x, y)
plt.plot(x, mymodel)
plt.show()
```

First, it uses stats.linregress to calculate the slope and intercept for the line of best fit. Second, it uses stats.pearsonr to calculate the Pearson correlation coefficient (corr), which numerically quantifies the strength and direction of the linear relationship. A correlation of 
−0.759 indicates a strong negative linear relationship, meaning that as the value of the xvariable increases, the value of the y variable tends to consistently decrease.

*Ex 3*

```python
import pandas
from sklearn import linear_model
df = pandas.read_csv("cars.csv")
X = df[['Weight', 'Volume']]
y = df['CO2']
regr = linear_model.LinearRegression()
regr.fit(X, y)
print(regr.coef_)
predictedCO2 = regr.predict([[3300, 1300]])
print(predictedCO2)
#predict the CO2 emission of a car where the weight is 2300kg, and the volume is 1300cm3:
predictedCO2 = regr.predict([[2300, 1300]])
print(predictedCO2)
```
Exercise 3 uses Multiple Linear Regression to model a car's CO2 emissions based on its weight and engine volume. It first trains a linear model by fitting 'Weight' and 'Volume' (the independent variables) to 'CO2' (the dependent variable). The resulting coefficients are printed to quantify the influence of each feature, and finally, the trained model is used to predict the CO2 emission for two different hypothetical cars with specified weight and volume values.

*Ex 4*

```python
import numpy
from sklearn.metrics import r2_score

x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]
y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]

mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))

speed = mymodel(10)
print(speed)
```
The script uses NumPy to fit a third-degree polynomial model to the given data points of time x and speed y. It then uses this established model to predict and print the expected speed y at the specific time of day x equal to 10.

## Unit 4
Unit 4 provided a practical application of foundational machine learning concepts, specifically focusing on correlation analysis and implementing linear regression using the scikit-learn library. The core exercise involved investigating the relationship between a country's mean population and its mean per capita Gross Domestic Product (GDP) over a two-decade period (2001–2021). This task directly addressed the module learning outcome concerning the applicability and challenges associated with different datasets for machine learning algorithms.

**Data Pre-processing:**
The analysis began with robust data pre-processing using the pandas library on the Global_GDP.csv and Global_Population.csv datasets. An important step was deriving the per capita GDP by element-wise division of the total GDP by the population for each country and year, followed by calculating the mean over the 20-year period (2001-2021).
The code snippet below illustrates the key steps for data loading, filtering, and calculating the independent and dependent variables:

```python
import pandas as pd

gdp_df = pd.read_csv("Global_GDP.csv")
pop_df = pd.read_csv("Global_Population.csv")

# Ensure unique entries and define years of interest
gdp_df.drop_duplicates(subset=['Country Name'], inplace=True, keep='first')
pop_df.drop_duplicates(subset=['Country Name'], inplace=True, keep='first')

years = [str(year) for year in range(2001, 2021)]
cols_to_keep = ['Country Name'] + years

# Filter, set index, and convert data types
gdp_filtered = gdp_df[cols_to_keep].set_index('Country Name')
pop_filtered = pop_df[cols_to_keep].set_index('Country Name')

for col in years:
    gdp_filtered[col] = pd.to_numeric(gdp_filtered[col], errors='coerce')
    pop_filtered[col] = pd.to_numeric(pop_filtered[col], errors='coerce')

# Calculate Per Capita GDP and Means
gdp_per_capita_yearly = gdp_filtered.div(pop_filtered)
mean_gdp_per_capita = gdp_per_capita_yearly.mean(axis=1)
mean_population = pop_filtered.mean(axis=1)

# Create final analysis DataFrame and handle missing values
analysis_df = pd.DataFrame({
    'Mean Population': mean_population,
    'Mean GDP per Capita': mean_gdp_per_capita
})
analysis_df.dropna(inplace=True)
```
Task A: Correlation Analysis
The first task involved investigating the linear relationship between the two key variables through a scatter plot and the calculation of the Pearson Correlation Coefficient.


The scatter plot visually suggested a non-linear or, at best, a very weak linear relationship. The plot showed that countries with the highest mean per capita GDP all had relatively low populations.
![Scatterplot](/MSc_AI_eportfolio/images/machine_learning/figure5.png "Scatterplot")

```python
correlation_coefficient = analysis_df['Mean Population'].corr(analysis_df['Mean GDP per Capita'])

print(f"Pearson Correlation Coefficient: {correlation_coefficient:.4f}")

```

```
Pearson Correlation Coefficient: -0.0990
```
This value, being very close to zero, confirms that there is no meaningful linear correlation between a country's mean population and its mean per capita GDP. This finding is significant because it suggests that Linear Regression may not be an appropriate predictive model for this specific relationship

Despite the poor correlation, a Linear Regression model was performed as an exercise to understand how the model fits the data and to derive the coefficients. Mean Population was set as the independent variable (X) and Mean GDP per Capita as the dependent variable (Y).
The scikit-learn library was used to set up and train the linear model:

```python
from sklearn.linear_model import LinearRegression
import numpy as np

# prepare data
X = analysis_df[['Mean Population']] 
y = analysis_df['Mean GDP per Capita']

# create and fit the model
model = LinearRegression()
model.fit(X, y)

# get the results
slope = model.coef_[0]
intercept = model.intercept_

print(f"Linear Regression Results:")
print(f"Slope (Coefficient): {slope:.4f}")
print(f"Y-Intercept: {intercept:.4f}")
print(f"\nThe equation of the regression line is: GDP_per_Capita = {slope:.4f} * Population + {intercept:.4f}")

```
![pairplot](/MSc_AI_eportfolio/images/machine_learning/figure6.png "Linear Regression")
```
Linear Regression Results:
Slope (Coefficient): -0.0000
Y-Intercept: 15253.5630

The equation of the regression line is: GDP_per_Capita = -0.0000 * Population + 15253.5630
```

The near-zero slope 0.000 mathematically confirms the finding from the correlation analysis: changes in a country's population size have a negligible impact on predicting its mean per capita GDP within a linear framework. The regression line, which is nearly horizontal at the value of the intercept, demonstrates the limitations of applying a linear model to non-linearly correlated data.
## Reflective Piece: What Have I Learned and How?
