---
title: "Machine Learning"
slug: "Machine Learning"
image: "./images/image3.jpg"
---
## Module Artefacts

### Unit 1
**Discussion 1 - Initial Post**

Drawing on Metcalf’s (2024) article, which frames Industry 5.0 as a human-centric correction to the techno-economic focus of Industry 4.0, I want to examine a catastrophic information system failure in the public administration sector: the UK’s Windrush scandal. This incident epitomises how implementing Industry 4.0 principles, such as automation and data-driven efficiency, without the ethical and human-centric safeguards of Industry 5.0, can lead to devastating societal harm.

The Windrush scandal involved the wrongful detention, denial of legal rights, and deportation of Commonwealth citizens; these individuals, known as the "Windrush generation", were legal residents and had been for decades (Dearden, 2018).  This was a direct consequence of the Home Office's "hostile environment" policy, which created a data-driven system designed to efficiently identify and penalise undocumented immigrants (Taylor, 2018).  The system operated on flawed logic: a lack of data in the system was treated as proof of illegal status. The burden of proof was shifted onto individuals, who were asked to provide official evidence for every year they had lived in the UK (an impossible demand for people who had arrived as children on their parents’ passports).

 The policy embodied an Industry 4.0 approach, prioritising automated data processing and efficiency above all else (Metcalf, 2024, p. 2), as it aimed to reduce a single metric: the number of undocumented immigrants. It is evident that it lacked the three core pillars of Industry 5.0: human-centricity, resilience, and sustainability (Metcalf, 2024, p. 2). The system was not human-centric, as it dehumanised citizens into data points and shifted an impossible burden of proof onto them. It was not resilient, as its rigid rules demanded a type of official paperwork that was known to be unavailable to the Windrush generation. Finally, it proved socially unsustainable by eroding public trust and causing deep, lasting harm to British communities.  The impact of this information system failure was severe. For the individuals affected, the consequences were life-changing: leading to wrongful detention, deportation, loss of homes, jobs, and access to healthcare, inflicting financial and psychological trauma. The economic cost includes a compensation scheme and the loss of these taxpayers’ economic contributions. Most significantly, the reputational cost to the UK Home Office and the government has been extensive, exposing what an independent review called a "profound institutional failure" (Williams 2020) and creating a legacy of systemic injustice.

**References**

Dearden, N. (2018). How British imperial policies led to the Windrush scandal. Al Jazeera Opinions. Available at: https://www.aljazeera.com/opinions/2018/4/19/how-british-imperial-policies-led-to-the-windrush-scandal (Accessed: 04 August 2025).

Joint Council for the Welfare of Immigrants. (n.d.). Windrush Scandal Explained. Available at https://jcwi.org.uk/reportsbriefings/windrush-scandal-explained/

Metcalf, G.S. (2024) 'An Introduction to Industry 5.0: History, Foundations, and Futures', In: Nousala, S., Metcalf, G., Ing, D. (eds) Industry 4.0 to Industry 5.0. Translational Systems Sciences, vol 41. Springer, Singapore.

Taylor, R. (2018). Impact of ‘Hostile Environment’ Policy: Debate on 14 June 2018 (House of Lords Library Briefing). House of Lords Library.

Williams, W. (2020) Windrush Lessons Learned Review. Available at: https://assets.publishing.service.gov.uk/media/5e74984fd3bf7f4684279faa/6.5577_HO_Windrush_Lessons_Learned_Review_WEB_v2.pdf  (Accessed: 04 August 2025).

**Link to Initial Post**: https://www.my-course.co.uk/mod/forum/discuss.php?d=312016
**Link to Peer responses**:

## Unit 2
The exploratory data analysis (EDA) steps detailed provide a understanding of the auto-mpg dataset, covering data cleaning, statistical properties, and inter-variable relationships.

```python
dataset = pd.read_csv("Unit02 auto-mpg.csv")
print(dataset.head())
print(dataset.describe())
dataset.info()
```
```
mpg             0.457092
cylinders       0.508109
displacement    0.701669
horsepower      1.087326
weight          0.519586
acceleration    0.291587
model year      0.019688
dtype: float64
mpg            -0.515993
cylinders      -1.398199
displacement   -0.778317
horsepower      0.696947
weight         -0.809259
acceleration    0.444234
model year     -1.167446
dtype: float64
```
 **Missing Values:** The 'horsepower' feature was found to contain the non-numeric string `'?'`. Rows with these missing values were dropped, and the 'horsepower' column was successfully converted to the correct `int` data type.
```python
dataset["horsepower"].unique()
```
```
array(['130', '165', '150', '140', '198', '220', '215', '225', '190',
       '170', '160', '95', '97', '85', '88', '46', '87', '90', '113',
       '200', '210', '193', '?', '100', '105', '175', '153', '180', '110',
       '72', '86', '70', '76', '65', '69', '60', '80', '54', '208', '155',
       '112', '92', '145', '137', '158', '167', '94', '107', '230', '49',
       '75', '91', '122', '67', '83', '78', '52', '61', '93', '148',
       '129', '96', '71', '98', '115', '53', '81', '79', '120', '152',
       '102', '108', '68', '58', '149', '89', '63', '48', '66', '139',
       '103', '125', '133', '138', '135', '142', '77', '62', '132', '84',
       '64', '74', '116', '82'], dtype=object)
```
```python
#dropping ?
rows_to_drop_mask = dataset["horsepower"] == "?"
dataset_filtered = dataset[~rows_to_drop_mask]
#convert horsepower to int
dataset_filtered.horsepower = dataset_filtered.horsepower.astype(int)
dataset_filtered.info()
```
**Feature Transformation:** The 'origin' variable was transformed from numerical codes (1, 2, 3) into the categorical labels **America, Europe, and Asia** for improved interpretability.

```python
#replace numeric values with categorical
origin_mapping = {1:"America",
                 2: "Europe",
                 3: "Asia"}
dataset_filtered["origin"] = dataset_filtered["origin"].map(origin_mapping)
```

Estimate skew and kurtosis:
```python
#estimate Skewness
skewness = dataset_filtered.skew(numeric_only=True)
#estimate kurtosis
kurtosis = numerical_cols.kurt(numeric_only=True)

print(skewness)
print (kurtosis)
```
```
mpg             0.457092
cylinders       0.508109
displacement    0.701669
horsepower      1.087326
weight          0.519586
acceleration    0.291587
model year      0.019688
dtype: float64
mpg            -0.515993
cylinders      -1.398199
displacement   -0.778317
horsepower      0.696947
weight         -0.809259
acceleration    0.444234
model year     -1.167446
dtype: float64
```
**Skewness and Kurtosis:** 'horsepower' exhibits a high positive skewness (1.09) and positive kurtosis (0.70), indicating a distribution with many low-value entries and a sharper peak than a normal distribution. Most other features, including 'displacement' (0.70) and 'weight' (0.52), also show a right-skew.

Create a correlation heatmap:

```python
# create correlation heatmap
correlation_matrix = dataset_filtered.corr(numeric_only=True)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix,
            annot=True,
            fmt=".2f",
            cmap='coolwarm',
            linewidths=1.5)

plt.title('Correlation Heatmap of Car Features')
plt.show()
```
![Correlation Heatmap of Car Features](/MSc_AI_eportfolio/images/machine_learning/figure1.png "Correlation Heatmap of Car Features")
*Figure 1*

A scatterplot pairwise visualisation of the relationships between all the numeric variables in the dataset:
```python
#scatterplots
sns.pairplot(dataset_filtered, hue='origin', markers=["o", "s", "D"])

plt.show()
```
![pairplot](/MSc_AI_eportfolio/images/machine_learning/figure2.png "Pairplot")
*Figure 2*

## Unit 3
This activity includes 4 Exercises in Jupyter Notebooks and to  change variables as needed to observe how the change in data points impacts correlation and regression.

**Ex1:**
```python
# calculate the Pearson's correlation between two variables
from numpy import mean
from numpy import std
from numpy import cov
from numpy.random import randn
from numpy.random import seed
from matplotlib import pyplot as plt
import seaborn as sns

from scipy.stats import pearsonr
# seed random number generator
seed(1)

# prepare data
data1 = 20 * randn(1000) + 100
data2 = data1 + (10 * randn(1000) + 50)

# calculate covariance matrix
covariance = cov(data1, data2)

# calculate Pearson's correlation
corr, _ = pearsonr(data1, data2)

# plot
plt.scatter(data1, data2)
plt.show()

# summarize
print('data1: mean=%.3f stdv=%.3f' % (mean(data1), std(data1)))
print('data2: mean=%.3f stdv=%.3f' % (mean(data2), std(data2)))
print('Covariance: %.3f' % covariance[0][1])
print('Pearsons correlation: %.3f' % corr)
```
![Scatterplot](/MSc_AI_eportfolio/images/machine_learning/figure3.png)
*Figure 3*
```
data1: mean=100.776 stdv=19.620
data2: mean=151.050 stdv=22.358
Covariance: 389.755
Pearsons correlation: 0.888
```
By removing the dependence on data1 and making data2 a completely independent random variable, there is no linear relationship between the two variables, pulling the Pearson's r close to 0.0.

`data2 = 50 * randn(1000) + 150`
![Scatterplot](/MSc_AI_eportfolio/images/machine_learning/figure4.png)
*Figure 4*

**Ex 2**

```python
import matplotlib.pyplot as plt
from scipy import stats

#Create the arrays that represent the values of the x and y axis
x = [5,7,8,7,2,17,2,9,4,11,12,9,6]
y = [99,86,87,88,111,86,103,87,94,78,77,85,86]

#Execute a method that returns some important key values of Linear Regression
slope, intercept, r, p, std_err = stat.linregress(x, y)

# measure the correlation
corr, _ = stat.pearsonr(x, y)
print('Pearsons correlation: %.3f' % corr)

#Create a function that uses the slope and intercept values to return a new value.
#This new value represents where on the y-axis the corresponding x value will be placed
def myfunc(x):
  return slope * x + intercept

#Run each value of the x array through the function. This will result in a new array with new values for the y-axis
mymodel = list(map(myfunc, x))

#Draw the original scatter plot & the line of linear regression
plt.scatter(x, y)
plt.plot(x, mymodel)
plt.show()
```

First, it uses stats.linregress to calculate the slope and intercept for the line of best fit. Second, it uses stats.pearsonr to calculate the Pearson correlation coefficient (corr), which numerically quantifies the strength and direction of the linear relationship. A correlation of
−0.759 indicates a strong negative linear relationship, meaning that as the value of the xvariable increases, the value of the y variable tends to consistently decrease.

**Ex 3**

```python
import pandas
from sklearn import linear_model
df = pandas.read_csv("cars.csv")
X = df[['Weight', 'Volume']]
y = df['CO2']
regr = linear_model.LinearRegression()
regr.fit(X, y)
print(regr.coef_)
predictedCO2 = regr.predict([[3300, 1300]])
print(predictedCO2)
#predict the CO2 emission of a car where the weight is 2300kg, and the volume is 1300cm3:
predictedCO2 = regr.predict([[2300, 1300]])
print(predictedCO2)
```
Exercise 3 uses Multiple Linear Regression to model a car's CO2 emissions based on its weight and engine volume. It first trains a linear model by fitting 'Weight' and 'Volume' (the independent variables) to 'CO2' (the dependent variable). The resulting coefficients are printed to quantify the influence of each feature, and finally, the trained model is used to predict the CO2 emission for two different hypothetical cars with specified weight and volume values.

**Ex 4**

```python
import numpy
from sklearn.metrics import r2_score

x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]
y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]

mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))

speed = mymodel(10)
print(speed)
```
The script uses NumPy to fit a third-degree polynomial model to the given data points of time x and speed y. It then uses this established model to predict and print the expected speed y at the specific time of day x equal to 10.

## Unit 4
Unit 4 provided a practical application of foundational machine learning concepts, specifically focusing on correlation analysis and implementing linear regression using the scikit-learn library. The core exercise involved investigating the relationship between a country's mean population and its mean per capita Gross Domestic Product (GDP) over a two-decade period (2001–2021). This task directly addressed the module learning outcome concerning the applicability and challenges associated with different datasets for machine learning algorithms.

**Data Pre-processing:**
The analysis began with robust data pre-processing using the pandas library on the Global_GDP.csv and Global_Population.csv datasets. An important step was deriving the per capita GDP by element-wise division of the total GDP by the population for each country and year, followed by calculating the mean over the 20-year period (2001-2021).
The code snippet below illustrates the key steps for data loading, filtering, and calculating the independent and dependent variables:

```python
import pandas as pd

gdp_df = pd.read_csv("Global_GDP.csv")
pop_df = pd.read_csv("Global_Population.csv")

# Ensure unique entries and define years of interest
gdp_df.drop_duplicates(subset=['Country Name'], inplace=True, keep='first')
pop_df.drop_duplicates(subset=['Country Name'], inplace=True, keep='first')

years = [str(year) for year in range(2001, 2021)]
cols_to_keep = ['Country Name'] + years

# Filter, set index, and convert data types
gdp_filtered = gdp_df[cols_to_keep].set_index('Country Name')
pop_filtered = pop_df[cols_to_keep].set_index('Country Name')

for col in years:
    gdp_filtered[col] = pd.to_numeric(gdp_filtered[col], errors='coerce')
    pop_filtered[col] = pd.to_numeric(pop_filtered[col], errors='coerce')

# Calculate Per Capita GDP and Means
gdp_per_capita_yearly = gdp_filtered.div(pop_filtered)
mean_gdp_per_capita = gdp_per_capita_yearly.mean(axis=1)
mean_population = pop_filtered.mean(axis=1)

# Create final analysis DataFrame and handle missing values
analysis_df = pd.DataFrame({
    'Mean Population': mean_population,
    'Mean GDP per Capita': mean_gdp_per_capita
})
analysis_df.dropna(inplace=True)
```
**Correlation Analysis**

The first task involved investigating the linear relationship between the two key variables through a scatter plot and the calculation of the Pearson Correlation Coefficient.


The scatter plot visually suggested a non-linear or, at best, a very weak linear relationship. The plot showed that countries with the highest mean per capita GDP all had relatively low populations.
![Scatterplot](/MSc_AI_eportfolio/images/machine_learning/figure5.png "Scatterplot")
*Figure 5*

```python
correlation_coefficient = analysis_df['Mean Population'].corr(analysis_df['Mean GDP per Capita'])

print(f"Pearson Correlation Coefficient: {correlation_coefficient:.4f}")

```

```
Pearson Correlation Coefficient: -0.0990
```
This value, being very close to zero, confirms that there is no meaningful linear correlation between a country's mean population and its mean per capita GDP. This finding is significant because it suggests that Linear Regression may not be an appropriate predictive model for this specific relationship

Despite the poor correlation, a Linear Regression model was performed as an exercise to understand how the model fits the data and to derive the coefficients. Mean Population was set as the independent variable (X) and Mean GDP per Capita as the dependent variable (Y).
The scikit-learn library was used to set up and train the linear model:

```python
from sklearn.linear_model import LinearRegression
import numpy as np

# prepare data
X = analysis_df[['Mean Population']]
y = analysis_df['Mean GDP per Capita']

# create and fit the model
model = LinearRegression()
model.fit(X, y)

# get the results
slope = model.coef_[0]
intercept = model.intercept_

print(f"Linear Regression Results:")
print(f"Slope (Coefficient): {slope:.4f}")
print(f"Y-Intercept: {intercept:.4f}")
print(f"\nThe equation of the regression line is: GDP_per_Capita = {slope:.4f} * Population + {intercept:.4f}")

```
![pairplot](/MSc_AI_eportfolio/images/machine_learning/figure6.png "Linear Regression")
```
Linear Regression Results:
Slope (Coefficient): -0.0000
Y-Intercept: 15253.5630

The equation of the regression line is: GDP_per_Capita = -0.0000 * Population + 15253.5630
```

The near-zero slope 0.000 mathematically confirms the finding from the correlation analysis: changes in a country's population size have a negligible impact on predicting its mean per capita GDP within a linear framework. The regression line, which is nearly horizontal at the value of the intercept, demonstrates the limitations of applying a linear model to non-linearly correlated data.

## Unit 5
 I implemented a method for calculating the Jaccard Distance between pairs of individuals based on their attributes.

 Each person's attributes are listed sequentially, where the first entry represents their Gender ("M" or "F"), and the subsequent entries are various characteristics denoted by codes like 'Y' (Yes), 'P' (Positive/Present), 'N' (No), and 'A' (Absent/Unknown). Since Jaccard Distance is a metric best suited for comparing sets based on shared presence of features, the code uses the convert_to_binary function to standardize the attributes: 'Y' and 'P' are mapped to the binary value 1 (indicating attribute presence), while all other values ('N', 'A', 'M', 'F') are mapped to 0 (indicating absence or being irrelevant for the comparison).

 ```python
 data = {
    "Jack": ["M", "Y", "N", "P", "N", "N", "A"],
    "Mary": ["F", "Y", "N", "P", "A", "P", "N"],
    "Jim":  ["M", "Y", "P", "N", "N", "N", "A"]
}

def convert_to_binary(value):
    if value in ['Y', 'P']:
        return 1
    return 0

def calculate_specific_jaccard_distance(person1_name, person2_name, data_dict):

    # get the full attribute lists
    list1_full = data_dict[person1_name]
    list2_full = data_dict[person2_name]

   # ignore gender as it's symetric
    list1 = list1_full[1:]
    list2 = list2_full[1:]

    f11, f10, f01 = 0, 0, 0

    for val1, val2 in zip(list1, list2):
        bin1 = convert_to_binary(val1)
        bin2 = convert_to_binary(val2)

        if bin1 == 1 and bin2 == 1:
            f11 += 1
        elif bin1 == 1 and bin2 == 0:
            f10 += 1
        elif bin1 == 0 and bin2 == 1:
            f01 += 1


    numerator = f01 + f10
    denominator = f01 + f10 + f11

    if denominator == 0:
        return 0.0

    return numerator / denominator

pairs_to_compare = [
    ("Jack", "Mary"),
    ("Jack", "Jim"),
    ("Jim", "Mary")
]

# print the results
print("Jaccard Distance Calculations:\n")

for p1, p2 in pairs_to_compare:
    coefficient = calculate_specific_jaccard_distance(p1, p2, data)
    print(f"({p1}, {p2}): {round(coefficient, 2)}")
```

```
Jaccard Distance Calculations:

(Jack, Mary): 0.33
(Jack, Jim): 0.67
(Jim, Mary): 0.75
```

## Unit 6
**Task A**

K-Means clustering analysis performed on the Iris dataset. The process involved determining the optimal number of clusters (K) using the Elbow Method and comparing the resulting clusters (at K=3) to the actual species labels.

The first step employed the Elbow Method to find the ideal number of clusters. This method plots the Within-Cluster Sum of Squares (WCSS) against the number of clusters (K). The plot, as shown in Figure 7, reveals a significant "elbow" at K=3, where the rate of WCSS reduction sharply decreases. This confirms that 3 is the optimal number of clusters, aligning with the three known species in the Iris dataset.

![Elbow Method](/MSc_AI_eportfolio/images/machine_learning/figure7.png "Elbow Method")
*Figure 7*

K-Means clustering was executed with (K=3)

Left Plot (K-Means Predicted Clusters): Shows the three clusters identified by the algorithm (Cluster 1, Cluster 2, Cluster 3) and their centroids.
Right Plot (Labels): Shows the data points colored by their true species: Iris-setosa, Iris-versicolour, and Iris-virginica.

![K-Means predicted vs Actual](/MSc_AI_eportfolio/images/machine_learning/figure8.png "K-Means predicted vs Actual")
*Figure 8*

The clusters show a strong correlation with the actual labels. Cluster 2 (red in K-Means plot) aligns nearly perfectly with Iris-setosa (red in Labels plot). Cluster 1 (blue) and Cluster 3 (green) generally correspond to Iris-versicolour (blue) and Iris-virginica (green), respectively. The minor overlap between these two predicted clusters reflects the natural proximity and overlap observed between the Versicolour and Virginica species in the true labels plot. The clustering successfully uncovered the inherent structure of the dataset.

**Task B**

Task B, which utilised the Wine dataset. The analysis followed three main steps: determining the optimal K, performing clustering at K=3, and evaluating the results using performance metrics and visualisations.

The initial step was to find the optimal number of clusters (K) by plotting the Within-Cluster Sum of Squares (WCSS). The WCSS plot, figure 9 displays a sharp drop in WCSS up to K=3, beyond K=3 the curve flattens significantly, forming a clear "elbow." This strongly indicates that K=3 is the appropriate number of clusters, which aligns with the three distinct types of wine (labels 1, 2, and 3) present in the dataset.

![K-Means predicted vs Actual](/MSc_AI_eportfolio/images/machine_learning/figure9.png "Elbow Method")
*Figure 9*

The model achieved exceptional performance metrics:
K-Means was executed with K=3 on the standardised features. Since K-Means assigns arbitrary cluster IDs, a label mapping technique (using the mode of the true labels within each cluster) was applied to align the predicted clusters with the true wine labels (1, 2, 3) for accurate evaluation.

Accuracy Score: 0.9663
High precision, recall, and F1-scores (all ≥ 0.95) across all three wine classes (1, 2, and 3), indicating the model successfully separated the wine types.

| Class | Precision | Recall | F1-Score | Support |
| :---: | :---: | :---: | :---: | :---: |
| **1** | 0.95 | 1.00 | 0.98 | 59 |
| **2** | 1.00 | 0.92 | 0.96 | 71 |
| **3** | 0.94 | 1.00 | 0.97 | 48 |
| | | | | |
| **Accuracy** | | | **0.97** | 178 |
| **Macro Avg** | 0.96 | 0.97 | 0.97 | 178 |
| **Weighted Avg** | 0.97 | 0.97 | 0.97 | 178 |

Confusion Matrix: The heatmap (figure 10) shows the high number of correct predictions along the diagonal, confirming the low misclassification rate. Only minor confusion is seen between the wine types.

![Wine Confusion Matrix](/MSc_AI_eportfolio/images/machine_learning/figure10.png "Wine Confusion Matrix")
*Figure 10*

PCA Scatter Plots: Principal Component Analysis (PCA) was used to reduce the 13 features to two components for a visual comparison. The side-by-side plots (figure 11) demonstrate:
Left Plot (True Labels): Shows the three distinct, well-separated wine groups.
Right Plot (K-Means Clustered Labels): Shows that the clusters identified by K-Means almost perfectly replicate the structure and separation of the true labels.

![PCA Scatter Plots](/MSc_AI_eportfolio/images/machine_learning/figure11.png "PCA Scatter Plots")
*Figure 11*

Python Code to produce plots:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.decomposition import PCA
from scipy.stats import mode
wine = pd.read_csv("Unit06 wine.csv")
X = wine.drop('Wine',axis=1)
y_true= wine['Wine']

#elbow method for idea number of k

wcss = []
for i in range (1,11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot the elbow method graph
plt.figure(figsize=(10, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method')
plt.xlabel('Number of clusters (K)')
plt.ylabel('WCSS')
plt.show()


# scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# perform K-Means clustering with k=3
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)
kmeans.fit(X_scaled)
y_pred_raw = kmeans.labels_

# map cluster labels to the true labels
labels = np.zeros_like(y_pred_raw)
for i in range(3):
    mask = (y_pred_raw == i)
    labels[mask] = mode(y_true[mask], keepdims=True)[0]

# 'labels' now holds the re-mapped cluster predictions
y_pred = labels

# evaluate the clustering performance
accuracy = accuracy_score(y_true, y_pred)
class_report = classification_report(y_true, y_pred)

print(f"Accuracy Score: {accuracy:.4f}\n")
print("Classification Report:")
print(class_report)

# plot the confusion matrix
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y_true),
            yticklabels=np.unique(y_true))
plt.title('Confusion Matrix for K-Means Clustering', fontsize=16)
plt.xlabel('Predicted Label', fontsize=12)
plt.ylabel('True Label', fontsize=12)
plt.show()

# perform PCA to reduce to 2 components for visualisation
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Create a DataFrame with the PCA components and labels
pca_df = pd.DataFrame(data=X_pca, columns=['Principal Component 1', 'Principal Component 2'])
pca_df['True Label'] = y_true
pca_df['Clustered Label'] = y_pred

# plot the clustered and the true labels side by side using PCA components
plt.figure(figsize=(14, 6))

# plotting the true labels
plt.subplot(1, 2, 1)
sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue='True Label', data=pca_df, palette='viridis', s=100)
plt.title('True Labels (PCA)', fontsize=14)
plt.xlabel('Principal Component 1', fontsize=12)
plt.ylabel('Principal Component 2', fontsize=12)
plt.grid(True)

# plotting the clustered labels
plt.subplot(1, 2, 2)
sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue='Clustered Label', data=pca_df, palette='viridis', s=100)
plt.title('K-Means Clustered Labels (PCA)', fontsize=14)
plt.xlabel('Principal Component 1', fontsize=12)
plt.ylabel('Principal Component 2', fontsize=12)
plt.grid(True)

plt.suptitle('Comparison of True and Clustered Labels using PCA', fontsize=18)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```
**Task C**

Task C involved performing K-Means clustering on the weatherAUS.csv dataset and visualizing the results for K=2 to K=6. Due to the multi-dimensional nature of the data, the clustering results are visualised using only two features, 'Maximum Temperature' (MaxTemp) and 'Minimum Temperature' (MinTemp), to observe how the algorithm separates the data across different K values.

![K=2](/MSc_AI_eportfolio/images/machine_learning/figure12.png "K=2")
![K=3](/MSc_AI_eportfolio/images/machine_learning/figure13.png "K=3")
![K=4](/MSc_AI_eportfolio/images/machine_learning/figure14.png "K=4")
![K=5](/MSc_AI_eportfolio/images/machine_learning/figure15.png "K=5")

## Unit 7
Unit 7 provided a practical introduction to neural networks by building perceptrons from scratch using NumPy. The exercises demonstrated a clear progression, starting with a simple perceptron, moving to a single-layer network for the linearly separable AND operator, and culminating in a multi-layer perceptron (MLP) to solve the non-linearly separable XOR problem.

### Exercise 1: Simple Perceptron with NumPy
This first exercise transitioned from a basic Python implementation to a more efficient NumPy-based approach. The key takeaway was the use of the `dot` product for calculating the weighted sum and a simple step function for activation. By adjusting the weights (e.g., from `[0.7, 0.1]` to `[-0.7, 0.1]`), the output was flipped from `1` to `0`, clearly demonstrating the direct impact of weights on the perceptron's decision.

### Exercise 2: Perceptron for the AND Operator
This notebook focused on training a single-layer perceptron to learn the linearly separable AND operator. Using the perceptron learning rule with a learning rate of `0.1`, the model iteratively adjusted its initial weights (`[0.0, 0.0]`) until the total error reached zero. After five updates, the final weights converged to `[0.5, 0.5]`, enabling the perceptron to correctly classify all four input combinations and proving its ability to learn a simple logical function.

### Exercise 3: Multi-Layer Perceptron for the XOR Problem
The final exercise addressed the non-linearly separable XOR problem by implementing a multi-layer perceptron (MLP) with one hidden layer. This required replacing the step function with the differentiable sigmoid function to enable backpropagation. The training process involved: **(1)** a forward pass to get the prediction and calculate the error, **(2)** backpropagation to compute the deltas for both the output and hidden layers, and **(3)** updating the weights for both layers (`weights_1` and `weights_0`). After 400,000 epochs, the model's error was significantly reduced, and it successfully learned to classify all XOR inputs, demonstrating the MLP's ability to handle complex, non-linear patterns that a single-layer perceptron cannot.

## Unit 8

**Discussion 2 - Initial Post**

The rise of sophisticated AI writers, as highlighted by Hutson (2021), presents a wide range of opportunities and challenges across various writing domain. These “robo-writers” offer significant benefits in efficiency and productivity, yet they also pose considerable risks to critical thinking and ethical standards.

For administrative work, the benefits of using LLMs are clear, these tools can streamline time-consuming tasks such as drafting emails, generating reports, and summarising documents. This automation can free up human workers to focus on more complex, strategic, and interpersonal aspects of their roles, potentially leading to increased productivity and innovation (Straub et al., 2024). In academic settings, language models are a transformative productivity tool including idea generation, content development, literature review, data analysis, publishing support, and ethical compliance while improving throughput, consistency and accessibility for non-native speakers (Khalifa and Albadawy, 2024).

However, as we move towards less routine and more creative applications, the risks associated with AI writers become more pronounced. In academic and professional writing, there is a danger of overreliance on these tools, which could hinder the development of essential writing and critical thinking skills (Neshkovska, 2025). Moreover, increased usage of AI to assist with grammar and structure may also lead to a homogenisation of writing styles and a noticeable reduction in diversity of thought. Furthermore, the potential for AI to generate plausible but inaccurate information raises concerns about the integrity of academic and research work.

For creative writing, significant ethical and legal challenges arise, particularly concerning the copyrighted materials on which these AI models are trained. Generative AI systems learn by analysing vast datasets of text and images, much of which is scraped from the internet and includes copyrighted books, articles, and other creative works (Kretschmer, Margoni & Oruç, 2024; Dornis & Stober, 2025). This practice of using protected works to train commercial AI models without permission, credit, or compensation has sparked intense debate and a wave of high-stakes lawsuits from authors, artists, and media companies who allege copyright infringement (Quang, 2021).

**References**

Dornis, T.W. & Stober, S. (2025) Generative AI Training and Copyright Law. arXiv preprint arXiv:2502.15858.

Fodouop Kouam, A.W. (2024) 'AI in Academic Writing: Ally or Foe?', International Journal of Research Publications, 148(1), pp. 353-358.

Hutson, M. (2021) Robo-writers: the rise and risks of language-generating AI.

Khalifa, M. and Albadawy, M. (2024) 'Using artificial intelligence in academic writing and research: An essential productivity tool', Computer Methods and Programs in Biomedicine Update, 5, 100145.

Kretschmer, M., Margoni, T. & Oruç, P. (2024) Copyright law and the lifecycle of machine learning models. IIC — International Review of Intellectual Property and Competition Law, 55, pp.110–138. doi:10.1007/s40319-023-01419-3.

Quang, T. (2021) ‘Does training AI violate copyright law?’, Berkeley Technology Law Journal, 36, pp.1407–1436.

Neshkovska, S. (2025) 'The Benefits and Risks of AI-Assisted Academic Writing: Insights from Current Research', ELOPE: English Language Overseas Perspectives and Enquiries, 22(1), pp. 55-68.

Straub, V. J., Hashem, Y., Bright, J., Bhagwanani, S., Morgan, D., Francis, J., Esnaashari, S., & Margetts, H. (2024). AI for bureaucratic productivity: Measuring the potential of AI to help automate 143 million UK government transactions. arXiv:2403.14712.

**Link to inital post** https://www.my-course.co.uk/mod/forum/discuss.php?d=323220


## Reflective Piece: What Have I Learned and How?

**Introduction**

This reflective account documents my learning journey through the Machine Learning module, featuring both theoretical foundations and practical implementation of complex algorithms. Adopting Rolfe et al.’s (2001) reflective model, the discussion is structured around three guiding questions: What? occurred, So what? it means, and Now what? I intend to do moving forward. This framework enables critical analysis of both technical progression and the development of professional and ethical awareness throughout the module.

**What?**

The Machine Learning module provided a comprehensive and practical introduction to the field, beginning with a crucial exploration of its societal impact. Unit 1’s analysis of the Windrush scandal highlighted the consequences that can arise when data-driven systems neglect human and ethical considerations. This discussion reinforced the notion that technical proficiency must be grounded in professional responsibility and social accountability; an idea strongly echoed in contemporary discussions of algorithmic bias and fairness (Noble, 2018; Bender et al., 2021).
The early units built a strong technical foundation, progressing from Exploratory Data Analysis (EDA) to understanding correlation, causality, and the limitations of linear regression. Later topics introduced key concepts such as Jaccard distance for similarity measurement, K-Means clustering for unsupervised learning, and the design and training of neural network architectures.

I applied these concepts in two assignments. The first was a group project predicting Airbnb prices in New York City, where I served as team leader. Our process involved data cleaning, feature engineering, hyperparameter tuning and the evaluation of several predictive models. XGBoost emerged as the most effective model, achieving an R-squared of 0.6231. The analysis identified room type and borough as the most significant determinants of price, leading us to propose a decision-support tool that could help hosts optimise their pricing and occupancy rates.

The module concluded with a solo project addressing CIFAR-10 image classification using two different approaches. The first approach involved developing a Convolutional Neural Network from scratch, utilising Keras Tuner with Hyperband to automate architecture optimisation. A three-block CNN with high dropout (0.6) achieved the best performance, reaching 81.38% test accuracy. The second approach employed transfer learning with a pre-trained Google Vision Transformer (ViT), achieving 96.12% accuracy after fine-tuning only the final layer. This improvement demonstrated the effectiveness of pre-trained architectures and the paradigm shift toward foundation models in modern machine learning (Dosovitskiy et al., 2021)

**So What?**

The integration of theory and practice was central to the value of this module. Leading the Airbnb project taught me as much about teamwork and leadership as it did about data science. My role required coordinating tasks, managing deadlines, and maintaining alignment across time zones, all while ensuring that our final report presented a coherent narrative. Most importantly, our model’s moderate explanatory power, an R-squared of 0.6231, offered a valuable reality check regarding model limitations and the bias–variance trade-off that constrains predictive accuracy (Geman, Bienenstock and Doursat, 1992).

The project provided hands-on experience of the full machine learning lifecycle: data preparation, feature selection, model tuning, and performance evaluation. This end-to-end exposure developed not only technical competence but also the soft skills required to manage a team and synthesise findings into actionable recommendations.

The solo CNN/ViT project marked a turning point in my technical development. Building a CNN from scratch deepened my understanding of model architecture, hyperparameter optimisation, and the practical challenges of computational efficiency. Yet its 81% accuracy, respectable but below modern benchmarks, illustrated the difficulty of achieving extremely high performance without extensive data and computational power. Examining the confusion matrix revealed that accuracy alone can obscure model weaknesses, reinforcing the need for more granular performance evaluation.

The Vision Transformer, by contrast, demonstrated the transformative potential of transfer learning and attention mechanisms (Vaswani et al., 2017). Achieving over 96% accuracy with minimal retraining showed the power of using pre-trained models, which make the most of large-scale learning from diverse datasets. This highlighted an important professional insight: modern machine learning often involves adaptation rather than constructing models entirely from scratch. In practice, success depends on balancing innovation with efficient use of existing resources, a concept closely aligned with Schön’s (1983) notion of the “reflective practitioner,” who learns through iterative experimentation and adaptation.


**Now What? Applying These Skills to Future Challenges**

This module has profoundly influenced both my technical direction and professional character. It strengthened my proficiency in Python, Scikit-learn, and deep learning frameworks such as Keras, while fostering a critical awareness of the ethical dimensions of AI. The early exploration of the Windrush scandal served as a reminder that algorithms are not neutral and that ethical deployment requires both transparency and accountability (Floridi and Cowls, 2019). This perspective will continue to shape my approach to developing and implementing machine learning systems responsibly.

The experience of leading a distributed team enhanced my ability to manage perspectives, coordinate across time zones, and translate data into business-relevant outcomes. These are transferable competencies that will prove essential in professional environments where collaboration and communication are key.

Technically, the success of the Vision Transformer sparked my deeper interest in computer vision and the rapid pace of change in modern foundation models. The dramatic performance improvement over the CNN highlighted a paradigm shift in machine learning practice, from model-centric to data- centric and architecture-centric approaches. I am particularly interested in exploring attention mechanisms and their application beyond image classification, including video analysis and temporal pattern recognition.

Looking ahead, I intend to pursue two key areas of development. First, I aim to concentrate on the challenges of model robustness and adaptability in production. The projects provided a strong foundation in building models on static datasets, but real-world data is constantly changing. I plan to study techniques for monitoring model performance over time and implementing strategies for continuous learning and retraining to ensure that systems remain accurate and fair long after their initial deployment. Second, I aim to gain hands-on experience with MLOps practices, including model versioning, monitoring, and deployment pipelines, to bridge the gap between research and production environments.

**References**

Bender, E.M., Gebru, T., McMillan-Major, A. and Shmitchell, S. (2021) ‘On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?’, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’21), pp. 610–623. New York: ACM. doi: 10.1145/3442188.3445922. Available at: https://dl.acm.org/doi/10.1145/3442188.3445922

Bishop, C.M. (2006) Pattern Recognition and Machine Learning. New York: Springer.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J. and Houlsby, N. (2021) ‘An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale’, International Conference on Learning Representations (ICLR) 2021. Available at: https://openreview.net/forum?id=YicbFdNTTy

Floridi, L. and Cowls, J. (2019) ‘A Unified Framework of Five Principles for AI in Society’, Harvard Data Science Review, 1(1). doi: 10.1162/99608f92.8cd550d1. Available at: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3831321

Geman, S., Bienenstock, E. and Doursat, R. (1992) ‘Neural Networks and the Bias/Variance Dilemma’, Neural Computation, 4(1), pp. 1–58. doi: 10.1162/neco.1992.4.1.1.

Noble, S.U. (2018) Algorithms of Oppression: How Search Engines Reinforce Racism. New York: NYU Press.

Rolfe, G., Freshwater, D. and Jasper, M. (2001) Critical Reflection in Nursing and the Helping Professions: A User’s Guide. Basingstoke: Palgrave Macmillan.

Schön, D.A. (1983) The Reflective Practitioner: How Professionals Think in Action. New York: Basic Books.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I. (2017) ‘Attention Is All You Need’, Advances in Neural Information Processing Systems (NeurIPS), 30, pp. 5998–6008. Available at: https://arxiv.org/abs/1706.03762
